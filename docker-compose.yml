version: '3.8'

services:
  vllm-router:
    build: .
    ports:
      - "8888:8888"
    volumes:
      - ./servers.toml:/app/servers.toml
      - ./logs:/app/logs
    environment:
      - PYTHONPATH=/app
      - LOG_LEVEL=INFO
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8888/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 40s
    networks:
      - vllm-network

  # Example vLLM servers (commented out - replace with your actual vLLM servers)
  # vllm-server-1:
  #   image: vllm/vllm-openai:latest
  #   ports:
  #     - "8000:8000"
  #   environment:
  #     - MODEL=meta-llama/Llama-2-7b-chat-hf
  #     - TOKENIZER=meta-llama/Llama-2-7b-chat-hf
  #   volumes:
  #     - ./models:/root/.cache/huggingface
  #   networks:
  #     - vllm-network
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: all
  #             capabilities: [gpu]

  # vllm-server-2:
  #   image: vllm/vllm-openai:latest
  #   ports:
  #     - "8001:8000"
  #   environment:
  #     - MODEL=meta-llama/Llama-2-7b-chat-hf
  #     - TOKENIZER=meta-llama/Llama-2-7b-chat-hf
  #   volumes:
  #     - ./models:/root/.cache/huggingface
  #   networks:
  #     - vllm-network
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: all
  #             capabilities: [gpu]

networks:
  vllm-network:
    driver: bridge

volumes:
  models: