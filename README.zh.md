# mvllm

**ç®¡ç†ä½ çš„ vLLM æœåŠ¡å™¨** - æ™ºèƒ½è´Ÿè½½å‡è¡¡å™¨ï¼Œç”¨äºåˆ†å¸ƒå¼ vLLM æœåŠ¡å™¨é›†ç¾¤

[![PyPI version](https://badge.fury.io/py/mvllm.svg)](https://badge.fury.io/py/mvllm)
[![GitHub stars](https://img.shields.io/github/stars/xerrors/mvllm?style=social)](https://github.com/xerrors/mvllm/stargazers)
[![GitHub license](https://img.shields.io/github/license/xerrors/mvllm)](https://github.com/xerrors/mvllm/blob/main/LICENSE)
[![Python versions](https://img.shields.io/pypi/pyversions/mvllm)](https://pypi.org/project/mvllm/)
[![Downloads](https://img.shields.io/pypi/dm/mvllm)](https://pypi.org/project/mvllm/)

## è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿ

å½“ä½ æœ‰å¤šä¸ª GPU æœåŠ¡å™¨è¿è¡Œ vLLM æ—¶ï¼Œä¼šé¢ä¸´ï¼š

- **èµ„æºåˆ†æ•£**ï¼šå¤šä¸ªç‹¬ç«‹çš„ GPU æ— æ³•ç»Ÿä¸€ç®¡ç†
- **è´Ÿè½½ä¸å‡**ï¼šæœ‰çš„æœåŠ¡å™¨è¿‡è½½ï¼Œæœ‰çš„ç©ºé—²
- **å¯ç”¨æ€§å·®**ï¼šå•ä¸ªæœåŠ¡å™¨æ•…éšœå½±å“æ•´ä½“æœåŠ¡

vLLM Router æä¾›ç»Ÿä¸€çš„å…¥å£ï¼Œæ™ºèƒ½åˆ†é…è¯·æ±‚åˆ°æœ€ä½³æœåŠ¡å™¨ã€‚

## æ ¸å¿ƒä¼˜åŠ¿

### ğŸ¯ æ™ºèƒ½è´Ÿè½½å‡è¡¡
- **å®æ—¶ç›‘æ§**ï¼šç›´æ¥è·å– vLLM çš„ `/metrics` æŒ‡æ ‡
- **æ™ºèƒ½ç®—æ³•**ï¼š`(running + waiting * 0.5) / capacity`
- **ä¼˜å…ˆçº§é€‰æ‹©**ï¼šä¼˜å…ˆé€‰æ‹©è´Ÿè½½ < 50% çš„æœåŠ¡å™¨
- **é›¶é˜Ÿåˆ—**ï¼šç›´æ¥è½¬å‘ï¼Œæ— ä¸­é—´é˜Ÿåˆ—

### ğŸ”„ é«˜å¯ç”¨æ€§
- **è‡ªåŠ¨æ•…éšœè½¬ç§»**ï¼šæ£€æµ‹å¹¶å‰”é™¤ä¸å¥åº·çš„æœåŠ¡å™¨
- **æ™ºèƒ½é‡è¯•**ï¼šè¯·æ±‚å¤±è´¥æ—¶è‡ªåŠ¨é‡è¯•å…¶ä»–æœåŠ¡å™¨
- **çƒ­é‡è½½**ï¼šé…ç½®ä¿®æ”¹æ— éœ€é‡å¯æœåŠ¡

### ğŸŒ OpenAI å…¼å®¹
- **æ— ç¼é›†æˆ**ï¼šå®Œå…¨å…¼å®¹ OpenAI API
- **é€šç”¨å®¢æˆ·ç«¯**ï¼šæ”¯æŒä»»ä½• OpenAI å…¼å®¹çš„å®¢æˆ·ç«¯

## å¿«é€Ÿå¼€å§‹

### å®‰è£…

#### é€‰é¡¹ 1ï¼šä» PyPI å®‰è£…ï¼ˆæ¨èï¼‰

```bash
pip install mvllm

mvllm run
```

#### é€‰é¡¹ 2ï¼šä»æºç å®‰è£…

```bash
git clone https://github.com/xerrors/mvllm.git
pip install -e .

mvllm run
```

### é…ç½®

åˆ›å»ºæœåŠ¡å™¨é…ç½®æ–‡ä»¶ï¼š

```bash
cp servers.example.toml servers.toml
```

ç¼–è¾‘ `servers.toml`ï¼š

```toml
[servers]
servers = [
    { url = "http://gpu-server-1:8081", max_concurrent_requests = 3 },
    { url = "http://gpu-server-2:8088", max_concurrent_requests = 5 },
    { url = "http://gpu-server-3:8089", max_concurrent_requests = 4 },
]

[config]
health_check_interval = 10
request_timeout = 120
max_retries = 3
```

### è¿è¡Œ

```bash
# ç”Ÿäº§æ¨¡å¼ï¼ˆå…¨å±ç›‘æ§ï¼‰
mvllm run

# å¼€å‘æ¨¡å¼ï¼ˆæ§åˆ¶å°æ—¥å¿—ï¼‰
mvllm run --console

# è‡ªå®šä¹‰ç«¯å£
mvllm run --port 8888
```

## ä½¿ç”¨ç¤ºä¾‹

### èŠå¤©å®Œæˆ

```bash
curl -X POST http://localhost:8888/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3.1:8b",
    "messages": [
      {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"}
    ]
  }'
```

### æŸ¥çœ‹è´Ÿè½½çŠ¶æ€

```bash
curl http://localhost:8888/health
curl http://localhost:8888/load-stats
```

## API ç«¯ç‚¹

- `POST /v1/chat/completions` - èŠå¤©å®Œæˆ
- `POST /v1/completions` - æ–‡æœ¬å®Œæˆ
- `GET /v1/models` - æ¨¡å‹åˆ—è¡¨
- `GET /health` - å¥åº·çŠ¶æ€
- `GET /load-stats` - è´Ÿè½½ç»Ÿè®¡

## éƒ¨ç½²

### Docker

```bash
docker build -t mvllm .
docker run -d -p 8888:8888 -v $(pwd)/servers.toml:/app/servers.toml mvllm
```

### Docker Compose

```yaml
version: '3.8'
services:
  mvllm:
    build: .
    ports:
      - "8888:8888"
    volumes:
      - ./servers.toml:/app/servers.toml
```

## é…ç½®è¯´æ˜

### æœåŠ¡å™¨é…ç½®
- `url`: vLLM æœåŠ¡å™¨åœ°å€
- `max_concurrent_requests`: æœ€å¤§å¹¶å‘è¯·æ±‚æ•°

### å…¨å±€é…ç½®
- `health_check_interval`: å¥åº·æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
- `request_timeout`: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
- `max_retries`: æœ€å¤§é‡è¯•æ¬¡æ•°

## ç›‘æ§

- **å®æ—¶è´Ÿè½½ç›‘æ§**ï¼šæ˜¾ç¤ºæ¯ä¸ªæœåŠ¡å™¨çš„è¿è¡Œå’Œç­‰å¾…è¯·æ±‚æ•°
- **å¥åº·çŠ¶æ€**ï¼šå®æ—¶ç›‘æ§æœåŠ¡å™¨å¯ç”¨æ€§
- **èµ„æºåˆ©ç”¨ç‡**ï¼šGPU ç¼“å­˜ä½¿ç”¨ç‡ç­‰æŒ‡æ ‡

## English Version

For English documentation, see [README.md](README.md)

## è®¸å¯è¯

MIT License