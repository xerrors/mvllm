# vLLM Router

<div align="center">

**ğŸš€ ä¼ä¸šçº§åˆ†å¸ƒå¼ vLLM æœåŠ¡å™¨è´Ÿè½½å‡è¡¡å™¨**

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Stars](https://img.shields.io/github/stars/xerrors/vllm-router?style=social)](https://github.com/xerrors/vllm-router)

**é€šè¿‡æ™ºèƒ½è´Ÿè½½å‡è¡¡è§£å†³åˆ†å¸ƒå¼ vLLM å®ä¾‹çš„ GPU ç¢ç‰‡åŒ–é—®é¢˜**

[å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹) â€¢ [æ ¸å¿ƒåŠŸèƒ½](#æ ¸å¿ƒåŠŸèƒ½) â€¢ [æ¶æ„è®¾è®¡](#æ¶æ„è®¾è®¡) â€¢ [æ–‡æ¡£](#æ–‡æ¡£)

</div>

---

## ğŸ¯ é—®é¢˜èƒŒæ™¯ï¼šLLM éƒ¨ç½²ä¸­çš„ GPU ç¢ç‰‡åŒ–

ç°ä»£å¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ Llama 3.1-8Bï¼‰éœ€è¦å¤§é‡ GPU èµ„æºï¼Œä½†ç»„ç»‡é€šå¸¸é¢ä¸´ä»¥ä¸‹æŒ‘æˆ˜ï¼š

- **åˆ†æ•£çš„ GPU èµ„æº**ï¼šå¤šå°æœºå™¨ä¸Šçš„ç‹¬ç«‹ GPU æ— æ³•æ•´åˆä½¿ç”¨
- **èµ„æºåˆ©ç”¨ç‡ä½**ï¼šéƒ¨åˆ† GPU è¿‡è½½è€Œå…¶ä»– GPU ç©ºé—²
- **ç®¡ç†å¤æ‚æ€§**ï¼šéœ€è¦æ‰‹åŠ¨åè°ƒå¤šä¸ª vLLM å®ä¾‹
- **å¯ç”¨æ€§æŒ‘æˆ˜**ï¼šå•ä¸ªèŠ‚ç‚¹ç»´æŠ¤æ—¶æœåŠ¡ä¸­æ–­

**vLLM Router é€šè¿‡ç»Ÿä¸€å…¥å£ç‚¹æ™ºèƒ½åˆ†é…è¯·æ±‚åˆ°æ‚¨çš„ vLLM é›†ç¾¤æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚**

---

## ğŸš€ æ ¸å¿ƒåŠŸèƒ½

### ğŸ”€ æ™ºèƒ½å¤šç«¯ç‚¹è´Ÿè½½å‡è¡¡
- **å®æ—¶æŒ‡æ ‡**ï¼šç›´æ¥é›†æˆ vLLM `/metrics` ç«¯ç‚¹
- **å®¹é‡æ„ŸçŸ¥è·¯ç”±**ï¼šåŒæ—¶è€ƒè™‘å½“å‰è´Ÿè½½å’ŒæœåŠ¡å™¨å®¹é‡
- **åŠ æƒåˆ†é…**ï¼šæ™ºèƒ½ç®—æ³•ï¼š`(è¿è¡Œä¸­ * 3 + ç­‰å¾…ä¸­) / å®¹é‡`
- **é›¶é˜Ÿåˆ—ç“¶é¢ˆ**ï¼šç›´æ¥è¯·æ±‚è½¬å‘ï¼Œæ— ä¸­é—´é˜Ÿåˆ—

### ğŸ“Š é«˜çº§ç›‘æ§ä¸å¯è§†åŒ–
- **å®æ—¶ä»ªè¡¨æ¿**ï¼šä½¿ç”¨ Rich æ§åˆ¶å°ç•Œé¢çš„å®æ—¶è´Ÿè½½ç›‘æ§
- **å…¨å±æ¨¡å¼**ï¼šæ§åˆ¶å°æ—¥å¿—ç¦ç”¨æ—¶çš„ä¸“ç”¨ç›‘æ§è§†å›¾
- **è¯¦ç»†æŒ‡æ ‡**ï¼šè¿è¡Œä¸­è¯·æ±‚ã€ç­‰å¾…é˜Ÿåˆ—ã€GPU ç¼“å­˜ä½¿ç”¨ç‡ã€æ–‡ä»¶æè¿°ç¬¦
- **å¥åº·çŠ¶æ€**ï¼šè‡ªåŠ¨å¥åº·æ£€æŸ¥å’Œæ•…éšœè½¬ç§»æ£€æµ‹

### âš¡ é«˜å¯ç”¨æ€§ä¸å¯é æ€§
- **è‡ªåŠ¨æ•…éšœè½¬ç§»**ï¼šå³æ—¶æ£€æµ‹å’Œç§»é™¤ä¸å¥åº·æœåŠ¡å™¨
- **æ–­è·¯å™¨æœºåˆ¶**ï¼šæ™ºèƒ½é‡è¯•é€»è¾‘é˜²æ­¢çº§è”æ•…éšœ
- **ä¼˜é›…é™çº§**ï¼šéƒ¨åˆ†æ•…éšœæœŸé—´ç»§ç»­æä¾›æœåŠ¡
- **å¯é…ç½®è¶…æ—¶**ï¼šç²¾ç»†çš„è¶…æ—¶å’Œé‡è¯•ç­–ç•¥

### ğŸ”§ é…ç½®ç®¡ç†
- **çƒ­é‡è½½**ï¼šæœåŠ¡ä¸ä¸­æ–­çš„é…ç½®æ›´æ–°
- **TOML é…ç½®**ï¼šäººç±»å¯è¯»çš„é…ç½®æ–‡ä»¶
- **ç¯å¢ƒå˜é‡**ï¼šçµæ´»çš„éƒ¨ç½²é€‰é¡¹
- **åŠ¨æ€æ‰©å±•**ï¼šæ— éœ€é‡å¯å³å¯æ·»åŠ /ç§»é™¤æœåŠ¡å™¨

### ğŸŒ OpenAI API å…¼å®¹æ€§
- **æ— ç¼é›†æˆ**ï¼šOpenAI API ç«¯ç‚¹çš„ç›´æ¥æ›¿ä»£å“
- **å®Œæ•´è¦†ç›–**ï¼šèŠå¤©è¡¥å…¨ã€æ–‡æœ¬è¡¥å…¨ã€åµŒå…¥ã€æ¨¡å‹åˆ—è¡¨
- **å®¢æˆ·ç«¯æ— å…³**ï¼šé€‚ç”¨äºä»»ä½• OpenAI å…¼å®¹çš„å®¢æˆ·ç«¯åº“

---

## ğŸ—ï¸ æ¶æ„æ¦‚è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    å®¢æˆ·ç«¯åº”ç”¨ç¨‹åº                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                   vLLM Router (è´Ÿè½½å‡è¡¡å™¨)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   è´Ÿè½½ç®¡ç†å™¨    â”‚  â”‚  å¥åº·ç›‘æ§å™¨     â”‚  â”‚ é…ç½®ç®¡ç†å™¨      â”‚ â”‚
â”‚  â”‚  - å®æ—¶æŒ‡æ ‡     â”‚  â”‚  - è‡ªåŠ¨ä¿®å¤     â”‚  â”‚  - çƒ­é‡è½½       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    vLLM æœåŠ¡å™¨é›†ç¾¤                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚ vLLM èŠ‚ç‚¹ 1 â”‚  â”‚ vLLM èŠ‚ç‚¹ 2 â”‚  â”‚ vLLM èŠ‚ç‚¹ N â”‚           â”‚
â”‚  â”‚ GPU: 1xRTX â”‚  â”‚ GPU: 2x4090 â”‚  â”‚ GPU: 1xA100 â”‚           â”‚
â”‚  â”‚ Llama-3.1-8Bâ”‚  â”‚ Llama-3.1-8Bâ”‚  â”‚ Mixtral-8x7Bâ”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒç»„ä»¶

1. **è´Ÿè½½ç®¡ç†å™¨**ï¼šå®æ—¶è´Ÿè½½ç›‘æ§å’ŒæœåŠ¡å™¨é€‰æ‹©
2. **å¥åº·ç›‘æ§å™¨**ï¼šæŒç»­å¥åº·æ£€æŸ¥å’Œè‡ªåŠ¨æ•…éšœè½¬ç§»
3. **é…ç½®ç®¡ç†å™¨**ï¼šåŠ¨æ€é…ç½®ç®¡ç†å’ŒéªŒè¯
4. **è¯·æ±‚è·¯ç”±å™¨**ï¼šå¸¦é‡è¯•é€»è¾‘çš„æ™ºèƒ½è¯·æ±‚åˆ†é…

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å‰ç½®è¦æ±‚

- Python 3.8+
- å¤šå°ä¸åŒæœºå™¨ä¸Šè¿è¡Œçš„ vLLM æœåŠ¡å™¨
- è·¯ç”±å™¨å’Œ vLLM æœåŠ¡å™¨ä¹‹é—´çš„ç½‘ç»œè¿é€šæ€§

### å®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/xerrors/vllm-router.git
cd vllm-router

# å®‰è£…ä¾èµ–
uv sync
```

### é…ç½®

åˆ›å»ºæ‚¨çš„æœåŠ¡å™¨é…ç½®ï¼š

```bash
cp servers.example.toml servers.toml
```

ç¼–è¾‘ `servers.toml` æ¥å®šä¹‰æ‚¨çš„ vLLM é›†ç¾¤ï¼š

```toml
[servers]
servers = [
    { url = "http://gpu-server-1:8081", max_concurrent_requests = 3 },
    { url = "http://gpu-server-2:8088", max_concurrent_requests = 5 },
    { url = "http://gpu-server-3:8089", max_concurrent_requests = 4 },
]

[config]
health_check_interval = 10
config_reload_interval = 30
request_timeout = 120
health_check_timeout = 5
max_retries = 3
```

### è¿è¡Œè·¯ç”±å™¨

#### ç”Ÿäº§æ¨¡å¼ï¼ˆå…¨å±ç›‘æ§ï¼‰
```bash
# ä¸“ç”¨ç›‘æ§è§†å›¾ï¼Œæ— æ§åˆ¶å°æ—¥å¿—
vllm-router run
```

#### å¼€å‘æ¨¡å¼ï¼ˆæ§åˆ¶å°æ—¥å¿—ï¼‰
```bash
# æ§åˆ¶å°è¾“å‡ºä¸ç›‘æ§æ··åˆ
vllm-router run --console
```

#### é«˜çº§é€‰é¡¹
```bash
# è‡ªå®šä¹‰ä¸»æœºå’Œç«¯å£
vllm-router run --host 0.0.0.0 --port 8888

# å¼€å‘ç¯å¢ƒè‡ªåŠ¨é‡è½½
vllm-router run --reload --console

# è‡ªå®šä¹‰é…ç½®
vllm-router run --config production-servers.toml
```

---

## ğŸ“¡ API ç«¯ç‚¹

### OpenAI å…¼å®¹ç«¯ç‚¹

æ‰€æœ‰ç«¯ç‚¹å®Œå…¨å…¼å®¹ OpenAI API è§„èŒƒï¼š

```bash
# èŠå¤©è¡¥å…¨
POST /v1/chat/completions

# æ–‡æœ¬è¡¥å…¨
POST /v1/completions

# æ¨¡å‹åˆ—è¡¨
GET /v1/models

# åµŒå…¥å‘é‡
POST /v1/embeddings
```

### ç®¡ç†ä¸ç›‘æ§ç«¯ç‚¹

```bash
# æœåŠ¡ä¿¡æ¯
GET /

# å¥åº·çŠ¶æ€å’Œé›†ç¾¤ç»Ÿè®¡
GET /health

# å®æ—¶è´Ÿè½½æŒ‡æ ‡å’Œåˆ©ç”¨ç‡
GET /load-stats
```

---

## ğŸ’» ä½¿ç”¨ç¤ºä¾‹

### èŠå¤©è¡¥å…¨

```bash
curl -X POST http://localhost:8888/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3.1:8b",
    "messages": [
      {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚"},
      {"role": "user", "content": "ç”¨ç®€å•çš„æœ¯è¯­è§£é‡Šé‡å­è®¡ç®—ã€‚"}
    ],
    "temperature": 0.7,
    "max_tokens": 500
  }'
```

### è´Ÿè½½ç»Ÿè®¡

```bash
curl -s http://localhost:8888/load-stats | jq '.'
```

**å“åº”ï¼š**
```json
{
  "servers": [
    {
      "url": "http://gpu-server-1:8081",
      "current_load": 2,
      "max_capacity": 3,
      "available_capacity": 1,
      "utilization_percent": 66.7,
      "status": true,
      "detailed_metrics": {
        "num_requests_running": 2,
        "num_requests_waiting": 0,
        "gpu_cache_usage_perc": 78.5,
        "process_max_fds": 65535
      }
    }
  ],
  "summary": {
    "total_servers": 3,
    "healthy_servers": 3,
    "total_active_load": 5,
    "total_capacity": 12,
    "overall_utilization_percent": 41.7
  }
}
```

---

## ğŸ³ éƒ¨ç½²é€‰é¡¹

### Docker éƒ¨ç½²

```bash
# æ„å»ºé•œåƒ
docker build -t vllm-router .

# ç”Ÿäº§æ¨¡å¼è¿è¡Œï¼Œå¸¦ç›‘æ§
docker run -d \
  --name vllm-router \
  -p 8888:8888 \
  -v $(pwd)/servers.toml:/app/servers.toml \
  vllm-router

# å¼€å‘æ¨¡å¼è¿è¡Œï¼Œå¸¦æ§åˆ¶å°
docker run -d \
  --name vllm-router \
  -p 8888:8888 \
  -v $(pwd)/servers.toml:/app/servers.toml \
  -e LOG_TO_CONSOLE=true \
  vllm-router run --console
```

### Docker Compose

```yaml
version: '3.8'
services:
  vllm-router:
    build: .
    ports:
      - "8888:8888"
    volumes:
      - ./servers.toml:/app/servers.toml
    environment:
      - LOG_LEVEL=INFO
    depends_on:
      - vllm-server-1
      - vllm-server-2

  vllm-server-1:
    image: vllm/vllm-openai:latest
    # ... vLLM æœåŠ¡å™¨é…ç½® ...

  vllm-server-2:
    image: vllm/vllm-openai:latest
    # ... vLLM æœåŠ¡å™¨é…ç½® ...
```

### Kubernetes éƒ¨ç½²

è¯·å‚é˜… `k8s/` ç›®å½•è·å–å®Œæ•´çš„ Kubernetes éƒ¨ç½²ç¤ºä¾‹ï¼ŒåŒ…æ‹¬ï¼š

- ä¾¿äºéƒ¨ç½²çš„ Helm charts
- æ°´å¹³ Pod è‡ªåŠ¨æ‰©å±•é…ç½®
- æœåŠ¡ç½‘æ ¼é›†æˆç¤ºä¾‹
- å¤šåŒºåŸŸéƒ¨ç½²ç­–ç•¥

---

## ğŸ”§ é…ç½®

### æœåŠ¡å™¨é…ç½®

```toml
[servers]
servers = [
    # æ¯ä¸ªæœåŠ¡å™¨å¯ä»¥æœ‰ä¸åŒçš„å®¹é‡
    { url = "http://server1:8081", max_concurrent_requests = 3 },
    { url = "http://server2:8088", max_concurrent_requests = 8 },
    { url = "http://server3:8089", max_concurrent_requests = 5 },
]

[config]
# å¥åº·æ£€æŸ¥è®¾ç½®
health_check_interval = 10          # æ¯ 10 ç§’æ£€æŸ¥ä¸€æ¬¡
health_check_timeout = 5             # 5 ç§’è¶…æ—¶
health_check_min_success_rate = 0.8 # éœ€è¦ 80% æˆåŠŸç‡
health_check_max_response_time = 2.0 # æœ€å¤§ 2 ç§’å“åº”æ—¶é—´

# é…ç½®ç®¡ç†
config_reload_interval = 30          # æ¯ 30 ç§’æ£€æŸ¥é…ç½®å˜æ›´
enable_active_health_check = true    # å¯ç”¨ä¸»åŠ¨å¥åº·ç›‘æ§

# è¯·æ±‚å¤„ç†
request_timeout = 120                # 2 åˆ†é’Ÿè¶…æ—¶
max_retries = 3                     # æœ€å¤šé‡è¯• 3 æ¬¡
retry_delay = 0.1                   # é‡è¯•é—´éš” 100ms
```

### ç¯å¢ƒå˜é‡

```bash
# åŸºæœ¬é…ç½®
CONFIG_PATH=/path/to/servers.toml    # é…ç½®æ–‡ä»¶è·¯å¾„
LOG_LEVEL=INFO                       # æ—¥å¿—çº§åˆ« (DEBUG, INFO, WARNING, ERROR)
LOG_TO_CONSOLE=false                 # å¯ç”¨æ§åˆ¶å°æ—¥å¿—
HOST=0.0.0.0                        # ç»‘å®šä¸»æœº
PORT=8888                           # ç»‘å®šç«¯å£

# é«˜çº§è®¾ç½®
HEALTH_CHECK_INTERVAL=10             # å¥åº·æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
CONFIG_RELOAD_INTERVAL=30            # é…ç½®é‡è½½é—´éš”ï¼ˆç§’ï¼‰
REQUEST_TIMEOUT=120                  # è¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰
```

---

## ğŸ“Š ç›‘æ§ä¸å¯è§‚æµ‹æ€§

### å®æ—¶ç›‘æ§

è·¯ç”±å™¨æä¾›å…¨é¢çš„ç›‘æ§åŠŸèƒ½ï¼š

**å…¨å±æ¨¡å¼**ï¼ˆå½“ `LOG_TO_CONSOLE=false` æ—¶ï¼‰ï¼š
- ä¸“ç”¨ç›‘æ§ç•Œé¢
- å®æ—¶è´Ÿè½½ç»Ÿè®¡
- æœåŠ¡å™¨å¥åº·çŠ¶æ€å¯è§†åŒ–
- æ¸…æ™°ä¸“ä¸šçš„æ˜¾ç¤º

**æ§åˆ¶å°æ¨¡å¼**ï¼ˆå½“ `LOG_TO_CONSOLE=true` æ—¶ï¼‰ï¼š
- æ··åˆæ§åˆ¶å°æ—¥å¿—å’Œç›‘æ§
- ä¼ ç»Ÿæ—¥å¿—ä½“éªŒ
- å¼€å‘å‹å¥½çš„è¾“å‡º

### æŒ‡æ ‡æ”¶é›†

- **vLLM é›†æˆ**ï¼šç›´æ¥ä» `/metrics` ç«¯ç‚¹è·å–æŒ‡æ ‡
- **è¯·æ±‚è·Ÿè¸ª**ï¼šæˆåŠŸç‡ã€å»¶è¿Ÿã€é”™è¯¯åˆ†å¸ƒ
- **èµ„æºç›‘æ§**ï¼šGPU ä½¿ç”¨ç‡ã€å†…å­˜ã€æ–‡ä»¶æè¿°ç¬¦
- **å¥åº·çŠ¶æ€**ï¼šæœåŠ¡å™¨å¯ç”¨æ€§ã€å“åº”æ—¶é—´ã€æˆåŠŸç‡

### æ—¥å¿—è®°å½•

```bash
# æ§åˆ¶å°è¾“å‡ºï¼ˆå¯ç”¨æ—¶ï¼‰
LOG_TO_CONSOLE=true vllm-router run

# æ–‡ä»¶æ—¥å¿—ï¼ˆå§‹ç»ˆå¯ç”¨ï¼‰
logs/vllm-router.log          # ä¸€èˆ¬åº”ç”¨æ—¥å¿—
logs/vllm-router-error.log    # ä»…é”™è¯¯æ—¥å¿—
logs/vllm-router-structured.log # æœºå™¨å¯è¯»æ—¥å¿—
```

---

## âš¡ æ€§èƒ½è°ƒä¼˜

### è´Ÿè½½å‡è¡¡ä¼˜åŒ–

æ™ºèƒ½è´Ÿè½½å‡è¡¡ç®—æ³•è€ƒè™‘ä»¥ä¸‹å› ç´ ï¼š

```python
# åˆ†æ•°è®¡ç®—ï¼š(running * 3 + waiting * 1) / capacity
# åˆ†æ•°è¶Šä½ = ä¸‹ä¸€ä¸ªè¯·æ±‚çš„æ›´å¥½å€™é€‰
```

**è°ƒä¼˜å»ºè®®**ï¼š
- **é«˜æ€§èƒ½é›†ç¾¤**ï¼šåœ¨å¼ºå¤§èŠ‚ç‚¹ä¸Šå¢åŠ  `max_concurrent_requests`
- **æ··åˆç¯å¢ƒ**ï¼šä¸ºè¾ƒæ—§/è¾ƒå¼±çš„ GPU è®¾ç½®è¾ƒä½å®¹é‡
- **ç¨³å®šæ€§é‡ç‚¹**ï¼šå‡å°‘ `max_retries` å¹¶å¢åŠ  `health_check_interval`

### é…ç½®ä¼˜åŒ–

```toml
# é«˜ååé‡é…ç½®
[config]
health_check_interval = 5           # é¢‘ç¹å¥åº·æ£€æŸ¥
request_timeout = 60                # è¾ƒä½è¶…æ—¶ä»¥æ›´å¿«æ•…éšœè½¬ç§»
max_retries = 2                     # è¾ƒå°‘é‡è¯•ä»¥æ›´å¿«å“åº”

# é«˜å¯é æ€§é…ç½®
[config]
health_check_interval = 15          # è¾ƒå°‘é¢‘ç¹æ£€æŸ¥
request_timeout = 180               # å¤æ‚æ¨¡å‹çš„è¾ƒé«˜è¶…æ—¶
max_retries = 5                     # æ›´å¤šé‡è¯•ä»¥æé«˜å¯é æ€§
```

### æ‰©å±•ç­–ç•¥

- **æ°´å¹³æ‰©å±•**ï¼šæ·»åŠ æ›´å¤š vLLM æœåŠ¡å™¨ä»¥æé«˜ååé‡
- **å‚ç›´æ‰©å±•**ï¼šå¢åŠ ç°æœ‰æœåŠ¡å™¨çš„ `max_concurrent_requests`
- **åœ°ç†åˆ†å¸ƒ**ï¼šåœ¨æ›´é è¿‘ç”¨æˆ·åŒºåŸŸçš„éƒ¨ç½²è·¯ç”±å™¨
- **å¤šå±‚æ¶æ„**ï¼šä¸ºä¸åŒæ¨¡å‹ç±»å‹/å¤§å°ä½¿ç”¨åˆ†ç¦»çš„è·¯ç”±å™¨

---

## ğŸ› ï¸ å¼€å‘

### è®¾ç½®å¼€å‘ç¯å¢ƒ

```bash
# å…‹éš†å’Œå®‰è£…
git clone https://github.com/xerrors/vllm-router.git
cd vllm-router
uv sync

# å®‰è£…å¼€å‘ä¾èµ–
uv add --dev pytest pytest-asyncio httpx black isort flake8

# è¿è¡Œæµ‹è¯•
uv run pytest

# ä»£ç è´¨é‡æ£€æŸ¥
black --check app/
isort --check-only app/
flake8 app/
```

### è¿è¡Œæµ‹è¯•

```bash
# æ‰€æœ‰æµ‹è¯•
uv run pytest

# å¸¦è¦†ç›–ç‡
uv run pytest --cov=app --cov-report=html

# ç‰¹å®šæµ‹è¯•ç±»åˆ«
uv run pytest tests/test_load_balancing.py
uv run pytest tests/test_health_monitoring.py
```

### è´¡çŒ®

æˆ‘ä»¬æ¬¢è¿è´¡çŒ®ï¼è¯·å‚é˜… [CONTRIBUTING.md](CONTRIBUTING.md) è·å–æŒ‡å—ã€‚

---

## ğŸ›ï¸ ç”Ÿäº§æœ€ä½³å®è·µ

### é«˜å¯ç”¨éƒ¨ç½²

1. **å¤šä¸ªè·¯ç”±å™¨å®ä¾‹**ï¼šåœ¨å¤šä¸ª vLLM Router å®ä¾‹å‰ä½¿ç”¨è´Ÿè½½å‡è¡¡å™¨
2. **å¥åº·ç›‘æ§**ï¼šå®æ–½å¸¦è‡ªåŠ¨æ•…éšœè½¬ç§»çš„å¤–éƒ¨å¥åº·æ£€æŸ¥
3. **æ•°æ®åº“æ”¯æŒ**ï¼šå°†é…ç½®å’ŒæŒ‡æ ‡å­˜å‚¨åœ¨æ•°æ®åº“ä¸­ä»¥å®ç°æŒä¹…åŒ–
4. **å‘Šè­¦**ï¼šä¸ºé«˜é”™è¯¯ç‡æˆ–æœåŠ¡å™¨ä¸å¯ç”¨è®¾ç½®å‘Šè­¦

### å®‰å…¨è€ƒè™‘

```bash
# ç”Ÿäº§å®‰å…¨è®¾ç½®
export LOG_LEVEL=WARNING               # å‡å°‘æ—¥å¿—è¯¦ç»†ç¨‹åº¦
export REQUEST_TIMEOUT=60              # è¾ƒä½è¶…æ—¶ä»¥æé«˜å®‰å…¨æ€§
export HEALTH_CHECK_INTERVAL=5         # é¢‘ç¹å¥åº·æ£€æŸ¥
# å¯ç”¨ HTTPSï¼ˆæ¨èï¼‰
# é…ç½®é˜²ç«å¢™è§„åˆ™
# å®æ–½èº«ä»½éªŒè¯/æˆæƒ
```

### ç›‘æ§æ ˆé›†æˆ

- **Prometheus**ï¼šå¯¼å‡ºæŒ‡æ ‡ç”¨äºç›‘æ§
- **Grafana**ï¼šä¸ºè´Ÿè½½å’Œæ€§èƒ½å¯è§†åŒ–åˆ›å»ºä»ªè¡¨æ¿
- **AlertManager**ï¼šè®¾ç½®æ™ºèƒ½å‘Šè­¦
- **ELK Stack**ï¼šé›†ä¸­å¼æ—¥å¿—è®°å½•å’Œåˆ†æ

---

## ğŸ¤ ç¤¾åŒºä¸æ”¯æŒ

### è·å–å¸®åŠ©

- ğŸ“– **æ–‡æ¡£**ï¼š[README.md](README.md)ã€[USAGE.md](USAGE.md)
- ğŸ› **é”™è¯¯æŠ¥å‘Š**ï¼š[GitHub Issues](https://github.com/xerrors/vllm-router/issues)
- ğŸ’¬ **è®¨è®º**ï¼š[GitHub Discussions](https://github.com/xerrors/vllm-router/discussions)
- ğŸ“§ **é‚®ä»¶æ”¯æŒ**ï¼šåˆ›å»º issue å¹¶æ ‡è®° "question" æ ‡ç­¾

### è´¡çŒ®

æˆ‘ä»¬é¼“åŠ±ç¤¾åŒºè´¡çŒ®ï¼æ— è®ºæ‚¨æ˜¯ï¼š

- ğŸ› **æŠ¥å‘Šé”™è¯¯**
- ğŸ’¡ **å»ºè®®åŠŸèƒ½**
- ğŸ“ **æ”¹è¿›æ–‡æ¡£**
- ğŸ‘¨â€ğŸ’» **æäº¤æ‹‰å–è¯·æ±‚**

æ¯ä¸ªè´¡çŒ®éƒ½æœ‰åŠ©äºè®© vLLM Router å˜å¾—æ›´å¥½ã€‚

### è·¯çº¿å›¾

- [ ] **v1.1**ï¼šè‡ªå®šä¹‰è´Ÿè½½å‡è¡¡ç®—æ³•çš„æ’ä»¶ç³»ç»Ÿ
- [ ] **v1.2**ï¼šé…ç½®å’Œç›‘æ§çš„ Web UI
- [ ] **v1.3**ï¼šé«˜çº§åˆ†æå’ŒæŠ¥å‘Š
- [ ] **v1.4**ï¼šå¤šåè®®æ”¯æŒï¼ˆgRPCã€WebSocketï¼‰
- [ ] **v2.0**ï¼šåˆ†å¸ƒå¼è·¯ç”±å™¨é›†ç¾¤

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

### ç¬¬ä¸‰æ–¹è®¸å¯è¯

- **FastAPI**ï¼šMIT è®¸å¯è¯
- **Rich**ï¼šMIT è®¸å¯è¯
- **aiohttp**ï¼šApache 2.0 è®¸å¯è¯
- **Click**ï¼šBSD è®¸å¯è¯

---

## ğŸ™ è‡´è°¢

- [vLLM](https://github.com/vllm-project/vllm) æä¾›å‡ºè‰²çš„ LLM æ¨ç†å¼•æ“
- [FastAPI](https://fastapi.tiangolo.com/) æä¾›ç°ä»£ Web æ¡†æ¶
- [Rich](https://github.com/Textualize/rich) æä¾›ç¾è§‚çš„ç»ˆç«¯åº”ç”¨ç¨‹åº
- å¼€æºç¤¾åŒºæä¾›çš„çµæ„Ÿå’Œåé¦ˆ

---

<div align="center">

**â­ å¦‚æœ vLLM Router å¸®åŠ©æ‚¨è§£å†³äº† GPU ç¢ç‰‡åŒ–æŒ‘æˆ˜ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ªæ˜Ÿæ ‡ï¼**

[![GitHub Stars](https://img.shields.io/github/stars/xerrors/vllm-router?style=social)](https://github.com/xerrors/vllm-router)

</div>