#!/usr/bin/env python3
"""
vLLM Router ä½¿ç”¨ç¤ºä¾‹é›†

è¿™ä¸ªæ–‡ä»¶åŒ…å«äº†å„ç§åœºæ™¯ä¸‹ä½¿ç”¨ vLLM Router çš„å®é™…ç¤ºä¾‹ï¼Œ
åŒ…æ‹¬åŸºæœ¬çš„ API è°ƒç”¨ã€é”™è¯¯å¤„ç†ã€æ€§èƒ½æµ‹è¯•ç­‰ã€‚
"""

import asyncio
import aiohttp
import json
import time
import random
from typing import List, Dict, Any, Optional

class VLLMRouterClient:
    """vLLM Router å®¢æˆ·ç«¯å°è£…ç±»"""

    def __init__(self, base_url: str = "http://localhost:8888"):
        self.base_url = base_url.rstrip('/')
        self.session: Optional[aiohttp.ClientSession] = None

    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=300)
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    async def health_check(self) -> Dict[str, Any]:
        """æ£€æŸ¥è·¯ç”±å™¨å¥åº·çŠ¶æ€"""
        async with self.session.get(f"{self.base_url}/health") as response:
            return await response.json()

    async def get_load_stats(self) -> Dict[str, Any]:
        """è·å–è´Ÿè½½ç»Ÿè®¡ä¿¡æ¯"""
        async with self.session.get(f"{self.base_url}/load-stats") as response:
            return await response.json()

    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        model: str = "llama3.1:8b",
        temperature: float = 0.7,
        max_tokens: int = 500,
        stream: bool = False
    ) -> Dict[str, Any]:
        """èŠå¤©è¡¥å…¨"""
        payload = {
            "model": model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": stream
        }

        async with self.session.post(
            f"{self.base_url}/v1/chat/completions",
            json=payload
        ) as response:
            return await response.json()

    async def text_completion(
        self,
        prompt: str,
        model: str = "llama3.1:8b",
        temperature: float = 0.7,
        max_tokens: int = 500
    ) -> Dict[str, Any]:
        """æ–‡æœ¬è¡¥å…¨"""
        payload = {
            "model": model,
            "prompt": prompt,
            "temperature": temperature,
            "max_tokens": max_tokens
        }

        async with self.session.post(
            f"{self.base_url}/v1/completions",
            json=payload
        ) as response:
            return await response.json()

    async def embeddings(
        self,
        input_text: str,
        model: str = "llama3.1:8b"
    ) -> Dict[str, Any]:
        """è·å–æ–‡æœ¬åµŒå…¥å‘é‡"""
        payload = {
            "model": model,
            "input": input_text
        }

        async with self.session.post(
            f"{self.base_url}/v1/embeddings",
            json=payload
        ) as response:
            return await response.json()

    async def list_models(self) -> Dict[str, Any]:
        """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
        async with self.session.get(f"{self.base_url}/v1/models") as response:
            return await response.json()


async def example_1_basic_usage():
    """ç¤ºä¾‹ 1: åŸºæœ¬ä½¿ç”¨æ–¹æ³•"""
    print("=== ç¤ºä¾‹ 1: åŸºæœ¬ä½¿ç”¨æ–¹æ³• ===")

    async with VLLMRouterClient() as client:
        # æ£€æŸ¥å¥åº·çŠ¶æ€
        health = await client.health_check()
        print(f"è·¯ç”±å™¨çŠ¶æ€: {health['status']}")
        print(f"å¥åº·æœåŠ¡å™¨æ•°: {health['healthy_servers']}/{health['total_servers']}")

        # è·å–è´Ÿè½½ç»Ÿè®¡
        stats = await client.get_load_stats()
        print(f"æ€»åˆ©ç”¨ç‡: {stats['summary']['overall_utilization_percent']:.1f}%")

        # ç®€å•èŠå¤©
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚"},
            {"role": "user", "content": "ç”¨ä¸€å¥è¯ä»‹ç»äººå·¥æ™ºèƒ½ã€‚"}
        ]

        response = await client.chat_completion(messages)
        print(f"AI å›å¤: {response['choices'][0]['message']['content']}")


async def example_2_concurrent_requests():
    """ç¤ºä¾‹ 2: å¹¶å‘è¯·æ±‚æµ‹è¯•"""
    print("\n=== ç¤ºä¾‹ 2: å¹¶å‘è¯·æ±‚æµ‹è¯• ===")

    async with VLLMRouterClient() as client:
        # å‡†å¤‡å¤šä¸ªè¯·æ±‚
        questions = [
            "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
            "è§£é‡Šæ·±åº¦å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ",
            "ç¥ç»ç½‘ç»œæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ",
            "ä»€ä¹ˆæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼Ÿ",
            "è®¡ç®—æœºè§†è§‰çš„åº”ç”¨æœ‰å“ªäº›ï¼Ÿ"
        ]

        # å¹¶å‘å‘é€è¯·æ±‚
        tasks = []
        for question in questions:
            messages = [
                {"role": "user", "content": question}
            ]
            tasks.append(client.chat_completion(messages, max_tokens=100))

        # ç­‰å¾…æ‰€æœ‰è¯·æ±‚å®Œæˆ
        start_time = time.time()
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        end_time = time.time()

        # åˆ†æç»“æœ
        success_count = sum(1 for r in responses if not isinstance(r, Exception))
        total_time = end_time - start_time

        print(f"å®Œæˆè¯·æ±‚æ•°: {success_count}/{len(questions)}")
        print(f"æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"å¹³å‡æ¯è¯·æ±‚: {total_time/len(questions):.2f}ç§’")

        # æ˜¾ç¤ºéƒ¨åˆ†ç»“æœ
        for i, (question, response) in enumerate(zip(questions, responses)):
            if not isinstance(response, Exception):
                answer = response['choices'][0]['message']['content']
                print(f"{i+1}. Q: {question}")
                print(f"   A: {answer[:50]}...")
                print()


async def example_3_error_handling():
    """ç¤ºä¾‹ 3: é”™è¯¯å¤„ç†å’Œé‡è¯•"""
    print("\n=== ç¤ºä¾‹ 3: é”™è¯¯å¤„ç†å’Œé‡è¯• ===")

    async with VLLMRouterClient() as client:
        # æµ‹è¯•ä¸åŒçš„é”™è¯¯æƒ…å†µ
        test_cases = [
            {
                "name": "æ­£å¸¸è¯·æ±‚",
                "messages": [{"role": "user", "content": "ä½ å¥½"}]
            },
            {
                "name": "ç©ºæ¶ˆæ¯",
                "messages": []
            },
            {
                "name": "è¶…é•¿æ¶ˆæ¯",
                "messages": [{"role": "user", "content": "A" * 10000}]
            },
            {
                "name": "æ— æ•ˆæ¨¡å‹",
                "messages": [{"role": "user", "content": "æµ‹è¯•"}],
                "model": "invalid-model"
            }
        ]

        for case in test_cases:
            try:
                response = await client.chat_completion(
                    messages=case["messages"],
                    model=case.get("model", "llama3.1:8b"),
                    max_tokens=50
                )
                print(f"âœ… {case['name']}: æˆåŠŸ")

            except Exception as e:
                print(f"âŒ {case['name']}: {str(e)}")


async def example_4_performance_benchmark():
    """ç¤ºä¾‹ 4: æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("\n=== ç¤ºä¾‹ 4: æ€§èƒ½åŸºå‡†æµ‹è¯• ===")

    async with VLLMRouterClient() as client:
        # æµ‹è¯•å‚æ•°
        concurrent_levels = [1, 5, 10, 20, 50]
        requests_per_level = 20

        results = []

        for concurrent in concurrent_levels:
            print(f"æµ‹è¯•å¹¶å‘çº§åˆ«: {concurrent}")

            # å‡†å¤‡è¯·æ±‚
            tasks = []
            for i in range(requests_per_level):
                messages = [
                    {"role": "user", "content": f"ç”Ÿæˆä¸€ä¸ªéšæœºæ•°å­—: {random.randint(1, 1000)}"}
                ]
                tasks.append(client.chat_completion(messages, max_tokens=20))

            # æ‰§è¡Œå¹¶å‘è¯·æ±‚
            start_time = time.time()
            responses = await asyncio.gather(*tasks, return_exceptions=True)
            end_time = time.time()

            # ç»Ÿè®¡ç»“æœ
            success_count = sum(1 for r in responses if not isinstance(r, Exception))
            total_time = end_time - start_time
            qps = success_count / total_time if total_time > 0 else 0

            result = {
                "concurrent": concurrent,
                "total_requests": requests_per_level,
                "success_count": success_count,
                "total_time": total_time,
                "qps": qps,
                "success_rate": success_count / requests_per_level * 100
            }
            results.append(result)

            print(f"  æˆåŠŸç‡: {result['success_rate']:.1f}%")
            print(f"  QPS: {result['qps']:.2f}")
            print(f"  æ€»è€—æ—¶: {total_time:.2f}ç§’")
            print()

        # æ˜¾ç¤ºæœ€ä½³æ€§èƒ½
        best_result = max(results, key=lambda x: x['qps'])
        print(f"ğŸ† æœ€ä½³æ€§èƒ½: å¹¶å‘ {best_result['concurrent']} æ—¶è¾¾åˆ° {best_result['qps']:.2f} QPS")


async def example_5_load_balancing_test():
    """ç¤ºä¾‹ 5: è´Ÿè½½å‡è¡¡æµ‹è¯•"""
    print("\n=== ç¤ºä¾‹ 5: è´Ÿè½½å‡è¡¡æµ‹è¯• ===")

    async with VLLMRouterClient() as client:
        # è·å–åˆå§‹è´Ÿè½½çŠ¶æ€
        initial_stats = await client.get_load_stats()
        print("åˆå§‹è´Ÿè½½çŠ¶æ€:")
        for server in initial_stats['servers']:
            print(f"  {server['url']}: {server['current_load']}/{server['max_capacity']} ({server['utilization_percent']:.1f}%)")

        # å‘é€ä¸€ç³»åˆ—è¯·æ±‚è§‚å¯Ÿè´Ÿè½½å˜åŒ–
        print("\nå‘é€ 50 ä¸ªè¯·æ±‚...")
        for i in range(50):
            messages = [
                {"role": "user", "content": f"æµ‹è¯•æ¶ˆæ¯ {i+1}: è¿™æ˜¯è´Ÿè½½å‡è¡¡æµ‹è¯•"}
            ]
            try:
                await client.chat_completion(messages, max_tokens=10)
                if (i + 1) % 10 == 0:
                    print(f"å·²å®Œæˆ {i+1} ä¸ªè¯·æ±‚")
            except Exception as e:
                print(f"è¯·æ±‚ {i+1} å¤±è´¥: {e}")

        # ç­‰å¾…è´Ÿè½½æ›´æ–°
        await asyncio.sleep(5)

        # è·å–æœ€ç»ˆè´Ÿè½½çŠ¶æ€
        final_stats = await client.get_load_stats()
        print("\næœ€ç»ˆè´Ÿè½½çŠ¶æ€:")
        for server in final_stats['servers']:
            print(f"  {server['url']}: {server['current_load']}/{server['max_capacity']} ({server['utilization_percent']:.1f}%)")


async def example_6_embeddings_and_similarity():
    """ç¤ºä¾‹ 6: åµŒå…¥å‘é‡å’Œç›¸ä¼¼åº¦è®¡ç®—"""
    print("\n=== ç¤ºä¾‹ 6: åµŒå…¥å‘é‡å’Œç›¸ä¼¼åº¦è®¡ç®— ===")

    async with VLLMRouterClient() as client:
        # æµ‹è¯•æ–‡æœ¬
        texts = [
            "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯",
            "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„å­é¢†åŸŸ",
            "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œ",
            "ä»Šå¤©å¤©æ°”å¾ˆå¥½",
            "æˆ‘å–œæ¬¢ç¼–ç¨‹"
        ]

        # è·å–åµŒå…¥å‘é‡
        embeddings = []
        for text in texts:
            try:
                response = await client.embeddings(text)
                embedding = response['data'][0]['embedding']
                embeddings.append(embedding)
                print(f"âœ… è·å–åµŒå…¥å‘é‡: {text[:30]}...")
            except Exception as e:
                print(f"âŒ è·å–åµŒå…¥å‘é‡å¤±è´¥: {e}")
                embeddings.append(None)

        # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆç®€å•ç¤ºä¾‹ï¼‰
        print("\næ–‡æœ¬ç›¸ä¼¼åº¦çŸ©é˜µ:")
        print("   " + " ".join(f"{i:2d}" for i in range(len(texts))))

        for i in range(len(texts)):
            if embeddings[i] is None:
                continue
            row = []
            for j in range(len(texts)):
                if embeddings[j] is None:
                    row.append(" N/A")
                    continue

                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                import numpy as np
                vec1 = np.array(embeddings[i])
                vec2 = np.array(embeddings[j])

                similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
                row.append(f"{similarity:3.2f}")

            print(f"{i:2d} " + " ".join(row))


async def example_7_streaming_chat():
    """ç¤ºä¾‹ 7: æµå¼èŠå¤©"""
    print("\n=== ç¤ºä¾‹ 7: æµå¼èŠå¤© ===")

    # æ³¨æ„ï¼šè¿™ä¸ªç¤ºä¾‹éœ€è¦ä¿®æ”¹å®¢æˆ·ç«¯ä»¥æ”¯æŒæµå¼å“åº”
    print("æµå¼èŠå¤©åŠŸèƒ½éœ€è¦é¢å¤–çš„å®¢æˆ·ç«¯æ”¯æŒï¼Œè¿™é‡Œæ¼”ç¤ºåŸºæœ¬æ¦‚å¿µã€‚")

    async with VLLMRouterClient() as client:
        messages = [
            {"role": "user", "content": "è¯·å†™ä¸€ä¸ªå…³äºæŠ€æœ¯çš„å°æ•…äº‹"}
        ]

        try:
            response = await client.chat_completion(
                messages,
                max_tokens=200,
                stream=False  # å½“å‰å®¢æˆ·ç«¯ä¸æ”¯æŒçœŸæ­£çš„æµå¼
            )

            content = response['choices'][0]['message']['content']
            print(f"å®Œæ•´å›å¤: {content}")

            # æ¨¡æ‹Ÿæµå¼è¾“å‡º
            print("\næ¨¡æ‹Ÿæµå¼è¾“å‡º:")
            words = content.split()
            for i, word in enumerate(words):
                if i > 0 and i % 5 == 0:
                    print()
                print(word + " ", end="", flush=True)
                await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿæ‰“å­—æ•ˆæœ
            print()

        except Exception as e:
            print(f"æµå¼èŠå¤©å¤±è´¥: {e}")


async def example_8_model_comparison():
    """ç¤ºä¾‹ 8: æ¨¡å‹æ¯”è¾ƒ"""
    print("\n=== ç¤ºä¾‹ 8: æ¨¡å‹æ¯”è¾ƒ ===")

    async with VLLMRouterClient() as client:
        # è·å–å¯ç”¨æ¨¡å‹
        try:
            models_response = await client.list_models()
            available_models = [model['id'] for model in models_response['data']]
            print(f"å¯ç”¨æ¨¡å‹: {available_models}")
        except Exception as e:
            print(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            # ä½¿ç”¨é»˜è®¤æ¨¡å‹
            available_models = ["llama3.1:8b"]

        # æµ‹è¯•é—®é¢˜
        test_question = "ä»€ä¹ˆæ˜¯é‡å­è®¡ç®—ï¼Ÿè¯·ç”¨ç®€å•çš„è¯­è¨€è§£é‡Šã€‚"

        # ä½¿ç”¨ä¸åŒæ¨¡å‹å›ç­”
        for model in available_models[:2]:  # åªæµ‹è¯•å‰ä¸¤ä¸ªæ¨¡å‹
            print(f"\nä½¿ç”¨æ¨¡å‹: {model}")
            print("-" * 50)

            try:
                start_time = time.time()
                response = await client.chat_completion(
                    [{"role": "user", "content": test_question}],
                    model=model,
                    max_tokens=150
                )
                end_time = time.time()

                answer = response['choices'][0]['message']['content']
                tokens_used = response['usage']['total_tokens']

                print(f"å›å¤: {answer}")
                print(f"è€—æ—¶: {end_time - start_time:.2f}ç§’")
                print(f"ä½¿ç”¨ tokens: {tokens_used}")

            except Exception as e:
                print(f"æ¨¡å‹ {model} æµ‹è¯•å¤±è´¥: {e}")


async def main():
    """è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("vLLM Router ä½¿ç”¨ç¤ºä¾‹é›†")
    print("=" * 50)

    examples = [
        example_1_basic_usage,
        example_2_concurrent_requests,
        example_3_error_handling,
        example_4_performance_benchmark,
        example_5_load_balancing_test,
        example_6_embeddings_and_similarity,
        example_7_streaming_chat,
        example_8_model_comparison
    ]

    for example in examples:
        try:
            await example()
        except Exception as e:
            print(f"ç¤ºä¾‹è¿è¡Œå¤±è´¥: {e}")
            print("è¯·ç¡®ä¿ vLLM Router æ­£åœ¨è¿è¡Œï¼Œå¹¶ä¸”æœ‰å¥åº·çš„åç«¯æœåŠ¡å™¨ã€‚")

        # ç¤ºä¾‹ä¹‹é—´æ·»åŠ é—´éš”
        await asyncio.sleep(1)

    print("\n" + "=" * 50)
    print("æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")


if __name__ == "__main__":
    # è¿è¡Œç¤ºä¾‹
    asyncio.run(main())